{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8330924f",
   "metadata": {},
   "source": [
    "\n",
    "# Phonetic Confusability Evaluation\n",
    "\n",
    "This notebook evaluates how a trained pitch extraction model handles phonetic voicing contrasts. It focuses on stops and fricatives that differ only in voicing (e.g., /b/ vs. /p/) and measures how often the model incorrectly predicts voicing or unvoicing for each phoneme. By sweeping across phonemes and plotting false-voicing and false-unvoicing rates, we can identify where the model begins to fail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8febad7",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "Uncomment and run the following cell if the dependencies used by this notebook are not installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b33e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld pandas tqdm seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c1b79",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and Global Configuration\n",
    "\n",
    "The repository root is appended to `sys.path` so the notebook can reuse utilities from the training codebase. Update the configuration dictionary with paths that are valid on your machine before running the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import math\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pyworld as pw\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (12, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98247730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"phoneme_metadata_csv\": REPO_ROOT / \"data\" / \"phoneme_segments.csv\",\n",
    "    \"audio_root\": REPO_ROOT / \"data\",\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"phoneme_column\": \"phoneme\",\n",
    "    \"start_column\": \"start_time\",\n",
    "    \"end_column\": \"end_time\",\n",
    "    \"audio_column\": \"audio_path\",\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16633862",
   "metadata": {},
   "source": [
    "\n",
    "## Helper Utilities\n",
    "\n",
    "The following helpers load the training configuration, instantiate the mel-spectrogram transform, resolve checkpoint paths, and handle model inference over arbitrary-length audio. The utilities mirror the approach used by the training pipeline so that evaluation is consistent with how the model was optimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db92956",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> Tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    MEL_PARAMS.setdefault(\"sample_rate\", DEFAULT_MEL_PARAMS[\"sample_rate\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = int(MEL_PARAMS.get(\"sample_rate\", DEFAULT_MEL_PARAMS[\"sample_rate\"]))\n",
    "print(f\"Target sample rate: {TARGET_SAMPLE_RATE} Hz\")\n",
    "\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "mel_transform.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    resolved_path: Optional[Path] = None\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        resolved_path = Path(checkpoint_path)\n",
    "    else:\n",
    "        config_checkpoint = CONFIG.get(\"checkpoint_path\")\n",
    "        if config_checkpoint:\n",
    "            resolved_path = Path(config_checkpoint)\n",
    "\n",
    "    if resolved_path is None:\n",
    "        checkpoint_dir = Path(CONFIG.get(\"checkpoint_dir\", REPO_ROOT / \"Checkpoint\"))\n",
    "        resolved_path = _latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "    if resolved_path is None or not resolved_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {resolved_path}\")\n",
    "\n",
    "    state = torch.load(resolved_path, map_location=DEVICE)\n",
    "    model_state = state.get(\"model\", state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\") if isinstance(model_state, dict) else None\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(state.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "\n",
    "    model = JDCNet(num_class=inferred_classes)\n",
    "    if isinstance(model_state, dict):\n",
    "        model.load_state_dict(model_state)\n",
    "    else:\n",
    "        model.load_state_dict(state)\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint: {resolved_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5445996",
   "metadata": {},
   "source": [
    "\n",
    "## Phoneme Metadata Loading\n",
    "\n",
    "Evaluation relies on a metadata table that lists the audio files and phoneme-aligned segments to analyze. The CSV file is expected to include at least the following columns:\n",
    "\n",
    "* `audio_path`: path to the waveform relative to `audio_root` (or absolute paths).\n",
    "* `phoneme`: the ARPAbet, IPA, or custom symbol for the segment.\n",
    "* `start_time` and `end_time`: segment bounds in seconds (optional; leave blank to use the full file).\n",
    "\n",
    "Additional columns are ignored but preserved in the returned DataFrame. Segments are filtered to the phoneme set defined in `TARGET_PHONEMES` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_PHONEMES: Dict[str, List[str]] = {\n",
    "    \"voiced\": [\"b\", \"d\", \"g\", \"v\", \"z\", \"ʒ\"],\n",
    "    \"unvoiced\": [\"p\", \"t\", \"k\", \"f\", \"s\", \"ʃ\"],\n",
    "}\n",
    "\n",
    "ALL_TARGET_PHONEMES = sorted({p for group in TARGET_PHONEMES.values() for p in group})\n",
    "\n",
    "\n",
    "def load_phoneme_metadata(path: Path) -> pd.DataFrame:\n",
    "    path = _resolve_relative_path(REPO_ROOT, path)\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"Phoneme metadata CSV not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    phoneme_col = CONFIG.get(\"phoneme_column\", \"phoneme\")\n",
    "    df = df[df[phoneme_col].isin(ALL_TARGET_PHONEMES)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows match the target phoneme set. Check the metadata file and configuration.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "phoneme_metadata = load_phoneme_metadata(CONFIG[\"phoneme_metadata_csv\"])\n",
    "phoneme_metadata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c537e5",
   "metadata": {},
   "source": [
    "\n",
    "## Audio and F0 Extraction Helpers\n",
    "\n",
    "To ensure consistent frame alignment between the ground-truth F0 (computed with WORLD) and the model predictions, the helpers below cache waveforms and F0 tracks, slice segments based on the metadata, and convert waveforms to mel spectrograms before running the network. Reference and predicted F0 sequences are trimmed to the same length prior to metric computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_AUDIO_CACHE: Dict[Path, Tuple[np.ndarray, int]] = {}\n",
    "_F0_CACHE: Dict[Path, np.ndarray] = {}\n",
    "\n",
    "\n",
    "def load_waveform(path: str | Path) -> Tuple[np.ndarray, int]:\n",
    "    resolved = _resolve_relative_path(CONFIG.get(\"audio_root\", REPO_ROOT), path)\n",
    "    if resolved in _AUDIO_CACHE:\n",
    "        return _AUDIO_CACHE[resolved]\n",
    "    if not resolved.is_file():\n",
    "        raise FileNotFoundError(f\"Audio file not found: {resolved}\")\n",
    "    audio, sr = sf.read(resolved)\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    audio = audio.astype(np.float32)\n",
    "    if sr != TARGET_SAMPLE_RATE:\n",
    "        audio = torchaudio.functional.resample(torch.from_numpy(audio), sr, TARGET_SAMPLE_RATE).numpy()\n",
    "        sr = TARGET_SAMPLE_RATE\n",
    "    _AUDIO_CACHE[resolved] = (audio, sr)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def load_audio_and_f0(path: str | Path) -> Tuple[np.ndarray, int, np.ndarray]:\n",
    "    resolved = _resolve_relative_path(CONFIG.get(\"audio_root\", REPO_ROOT), path)\n",
    "    audio, sr = load_waveform(resolved)\n",
    "    if resolved in _F0_CACHE:\n",
    "        return audio, sr, _F0_CACHE[resolved]\n",
    "    f0 = compute_world_f0(audio, sr)\n",
    "    _F0_CACHE[resolved] = f0\n",
    "    return audio, sr, f0\n",
    "\n",
    "\n",
    "def compute_world_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    frame_period = MEL_PARAMS[\"hop_length\"] * 1000.0 / sr\n",
    "    f0, t = pw.harvest(audio.astype(np.float64), sr, frame_period=frame_period)\n",
    "    if np.count_nonzero(f0) < 5:\n",
    "        f0, t = pw.dio(audio.astype(np.float64), sr, frame_period=frame_period)\n",
    "    refined = pw.stonemask(audio.astype(np.float64), f0, t, sr)\n",
    "    return refined.astype(np.float32)\n",
    "\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = int(CONFIG.get(\"chunk_size\", 192))\n",
    "    overlap = int(CONFIG.get(\"chunk_overlap\", 48))\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def slice_segment(audio: np.ndarray, sr: int, start: Optional[float], end: Optional[float]) -> np.ndarray:\n",
    "    if start is None and end is None:\n",
    "        return audio\n",
    "    start_idx = 0 if start is None or math.isnan(start) else int(max(start * sr, 0))\n",
    "    end_idx = len(audio) if end is None or math.isnan(end) else int(min(end * sr, len(audio)))\n",
    "    if end_idx <= start_idx:\n",
    "        end_idx = min(start_idx + sr // MEL_PARAMS.get(\"hop_length\", 300), len(audio))\n",
    "    return audio[start_idx:end_idx]\n",
    "\n",
    "\n",
    "def extract_reference_segment(f0: np.ndarray, sr: int, start: Optional[float], end: Optional[float]) -> np.ndarray:\n",
    "    hop = MEL_PARAMS.get(\"hop_length\", 300)\n",
    "    frame_period = hop / sr\n",
    "    start_frame = 0 if start is None or math.isnan(start) else int(max(math.floor(start / frame_period), 0))\n",
    "    end_frame = len(f0) if end is None or math.isnan(end) else int(min(math.ceil(end / frame_period), len(f0)))\n",
    "    if end_frame <= start_frame:\n",
    "        end_frame = min(start_frame + 1, len(f0))\n",
    "    return f0[start_frame:end_frame]\n",
    "\n",
    "\n",
    "def align_reference_to_prediction(reference: np.ndarray, prediction: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    length = min(len(reference), len(prediction))\n",
    "    if length <= 0:\n",
    "        return reference, prediction\n",
    "    return reference[:length], prediction[:length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaac26",
   "metadata": {},
   "source": [
    "\n",
    "## Metric Computation\n",
    "\n",
    "False-voicing and false-unvoicing rates are computed on a per-phoneme basis. A \"false-voicing\" event occurs when the ground-truth segment is unvoiced (F0 == 0 Hz) but the model predicts a voiced frame. Conversely, \"false-unvoicing\" counts voiced ground-truth frames where the model predicts silence or an unvoiced frame. Rates are normalized by the number of reference frames belonging to each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3231950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class SegmentResult:\n",
    "    phoneme: str\n",
    "    category: str\n",
    "    total_frames: int\n",
    "    voiced_frames: int\n",
    "    unvoiced_frames: int\n",
    "    false_voicing: int\n",
    "    false_unvoicing: int\n",
    "    false_voicing_rate: float\n",
    "    false_unvoicing_rate: float\n",
    "\n",
    "\n",
    "def compute_segment_metrics(phoneme: str, reference: np.ndarray, prediction: np.ndarray) -> SegmentResult:\n",
    "    reference, prediction = align_reference_to_prediction(reference, prediction)\n",
    "    total_frames = int(min(len(reference), len(prediction)))\n",
    "    if total_frames == 0:\n",
    "        return SegmentResult(\n",
    "            phoneme=phoneme,\n",
    "            category=_categorize_phoneme(phoneme),\n",
    "            total_frames=0,\n",
    "            voiced_frames=0,\n",
    "            unvoiced_frames=0,\n",
    "            false_voicing=0,\n",
    "            false_unvoicing=0,\n",
    "            false_voicing_rate=float(\"nan\"),\n",
    "            false_unvoicing_rate=float(\"nan\"),\n",
    "        )\n",
    "\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG.get(\"voicing_threshold_hz\", 10.0)\n",
    "\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    unvoiced_frames = total_frames - voiced_frames\n",
    "\n",
    "    false_voicing = int(np.count_nonzero(~ref_voiced & pred_voiced))\n",
    "    false_unvoicing = int(np.count_nonzero(ref_voiced & ~pred_voiced))\n",
    "\n",
    "    false_voicing_rate = float(false_voicing / unvoiced_frames) if unvoiced_frames > 0 else float(\"nan\")\n",
    "    false_unvoicing_rate = float(false_unvoicing / voiced_frames) if voiced_frames > 0 else float(\"nan\")\n",
    "\n",
    "    return SegmentResult(\n",
    "        phoneme=phoneme,\n",
    "        category=_categorize_phoneme(phoneme),\n",
    "        total_frames=total_frames,\n",
    "        voiced_frames=voiced_frames,\n",
    "        unvoiced_frames=unvoiced_frames,\n",
    "        false_voicing=false_voicing,\n",
    "        false_unvoicing=false_unvoicing,\n",
    "        false_voicing_rate=false_voicing_rate,\n",
    "        false_unvoicing_rate=false_unvoicing_rate,\n",
    "    )\n",
    "\n",
    "\n",
    "def _categorize_phoneme(phoneme: str) -> str:\n",
    "    for category, phones in TARGET_PHONEMES.items():\n",
    "        if phoneme in phones:\n",
    "            return category\n",
    "    return \"other\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffd3ba",
   "metadata": {},
   "source": [
    "\n",
    "## Run Phoneme Sweep\n",
    "\n",
    "The loop below iterates over every segment listed in the metadata file, runs the model, and aggregates the per-segment metrics into a DataFrame. The resulting table contains one row per segment, which can be grouped or pivoted to analyze error patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc72b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results: List[SegmentResult] = []\n",
    "phoneme_col = CONFIG.get(\"phoneme_column\", \"phoneme\")\n",
    "start_col = CONFIG.get(\"start_column\", \"start_time\")\n",
    "end_col = CONFIG.get(\"end_column\", \"end_time\")\n",
    "audio_col = CONFIG.get(\"audio_column\", \"audio_path\")\n",
    "\n",
    "for _, row in tqdm(phoneme_metadata.iterrows(), total=len(phoneme_metadata), desc=\"Evaluating segments\"):\n",
    "    phoneme = str(row[phoneme_col])\n",
    "    start_time = row.get(start_col)\n",
    "    end_time = row.get(end_col)\n",
    "    start_time = None if pd.isna(start_time) else float(start_time)\n",
    "    end_time = None if pd.isna(end_time) else float(end_time)\n",
    "\n",
    "    audio, sr, f0_track = load_audio_and_f0(row[audio_col])\n",
    "\n",
    "    segment_audio = slice_segment(audio, sr, start_time, end_time)\n",
    "    reference_segment = extract_reference_segment(f0_track, sr, start_time, end_time)\n",
    "\n",
    "    prediction = predict_f0(model, segment_audio)\n",
    "\n",
    "    segment_result = compute_segment_metrics(phoneme, reference_segment, prediction)\n",
    "    results.append(segment_result)\n",
    "\n",
    "results_df = pd.DataFrame([r.__dict__ for r in results])\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ee7d5",
   "metadata": {},
   "source": [
    "\n",
    "## Aggregate Metrics by Phoneme\n",
    "\n",
    "Aggregating the per-segment results reveals systematic voicing errors for each phoneme. The summary statistics below average the false-voicing and false-unvoicing rates across all segments belonging to the same phoneme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebe3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phoneme_summary = (\n",
    "    results_df.groupby([\"phoneme\", \"category\"])\n",
    "    .agg({\n",
    "        \"total_frames\": \"sum\",\n",
    "        \"voiced_frames\": \"sum\",\n",
    "        \"unvoiced_frames\": \"sum\",\n",
    "        \"false_voicing\": \"sum\",\n",
    "        \"false_unvoicing\": \"sum\",\n",
    "        \"false_voicing_rate\": \"mean\",\n",
    "        \"false_unvoicing_rate\": \"mean\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "phoneme_summary.sort_values(\"phoneme\", inplace=True)\n",
    "phoneme_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea69d9",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization\n",
    "\n",
    "False-voicing and false-unvoicing rates are plotted against the phoneme condition levels. This view highlights which phonemes are most prone to voicing confusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_phonemes = [p for group in TARGET_PHONEMES.values() for p in group]\n",
    "plot_df = phoneme_summary.set_index(\"phoneme\").reindex(ordered_phonemes).reset_index()\n",
    "plot_df.rename(columns={\"index\": \"phoneme\"}, inplace=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharey=False)\n",
    "\n",
    "valid_false_voicing = plot_df[\"false_voicing_rate\"].dropna()\n",
    "max_false_voicing = float(valid_false_voicing.max()) if not valid_false_voicing.empty else 0.0\n",
    "\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"phoneme\",\n",
    "    y=\"false_voicing_rate\",\n",
    "    hue=\"category\",\n",
    "    palette=\"Blues\",\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"False-Voicing Rate by Phoneme\")\n",
    "axes[0].set_ylabel(\"False-Voicing Rate\")\n",
    "axes[0].set_xlabel(\"Phoneme\")\n",
    "axes[0].legend(title=\"Category\")\n",
    "axes[0].set_ylim(0, min(1.0, max(0.01, max_false_voicing * 1.1 if max_false_voicing > 0 else 0.1)))\n",
    "\n",
    "valid_false_unvoicing = plot_df[\"false_unvoicing_rate\"].dropna()\n",
    "max_false_unvoicing = float(valid_false_unvoicing.max()) if not valid_false_unvoicing.empty else 0.0\n",
    "\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"phoneme\",\n",
    "    y=\"false_unvoicing_rate\",\n",
    "    hue=\"category\",\n",
    "    palette=\"Reds\",\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"False-Unvoicing Rate by Phoneme\")\n",
    "axes[1].set_ylabel(\"False-Unvoicing Rate\")\n",
    "axes[1].set_xlabel(\"Phoneme\")\n",
    "axes[1].legend(title=\"Category\")\n",
    "axes[1].set_ylim(0, min(1.0, max(0.01, max_false_unvoicing * 1.1 if max_false_unvoicing > 0 else 0.1)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e3dcd",
   "metadata": {},
   "source": [
    "\n",
    "## Category-Level Summary\n",
    "\n",
    "The table below aggregates metrics by voiced vs. unvoiced classes to expose broader trends across the phoneme groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac34f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_summary = (\n",
    "    results_df.groupby(\"category\")\n",
    "    .agg({\n",
    "        \"total_frames\": \"sum\",\n",
    "        \"voiced_frames\": \"sum\",\n",
    "        \"unvoiced_frames\": \"sum\",\n",
    "        \"false_voicing\": \"sum\",\n",
    "        \"false_unvoicing\": \"sum\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "category_summary[\"false_voicing_rate\"] = category_summary[\"false_voicing\"] / category_summary[\"unvoiced_frames\"].replace({0: np.nan})\n",
    "category_summary[\"false_unvoicing_rate\"] = category_summary[\"false_unvoicing\"] / category_summary[\"voiced_frames\"].replace({0: np.nan})\n",
    "category_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0764b30",
   "metadata": {},
   "source": [
    "\n",
    "## Save Results\n",
    "\n",
    "All intermediate tables are saved to the configured `output_dir` so they can be compared across checkpoints or combined with other evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = CONFIG[\"output_dir\"]\n",
    "phoneme_summary_path = output_dir / \"phoneme_confusability_summary.csv\"\n",
    "segment_results_path = output_dir / \"phoneme_confusability_segments.csv\"\n",
    "category_summary_path = output_dir / \"phoneme_confusability_category.csv\"\n",
    "\n",
    "phoneme_summary.to_csv(phoneme_summary_path, index=False)\n",
    "results_df.to_csv(segment_results_path, index=False)\n",
    "category_summary.to_csv(category_summary_path, index=False)\n",
    "\n",
    "phoneme_summary_path, segment_results_path, category_summary_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ed031",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "\n",
    "* Compare multiple checkpoints by pointing `checkpoint_path` to different models and re-running the notebook.\n",
    "* Expand the phoneme list to include additional minimal pairs or language-specific contrasts.\n",
    "* Join the per-segment metrics with acoustic metadata (e.g., SNR, speaker identity) to understand when voicing errors are most likely.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
