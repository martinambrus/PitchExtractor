{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8534ac9",
   "metadata": {},
   "source": [
    "# Dynamic Pitch Behavior Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225f49e",
   "metadata": {},
   "source": [
    "This notebook evaluates how the latest pitch extraction model responds to dynamic pitch behaviors such as vibrato and portamento/glide transitions. Synthetic stimuli are generated with controlled parameters so that we can quantify accuracy, latency, and overshoot across a range of condition levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld torchcrepe rmvpe praat-parselmouth pyreaper pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613eb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "from Utils.dynamic_pitch_tools import (\n",
    "    generate_vibrato_waveform,\n",
    "    generate_glide_waveform,\n",
    "    sample_reference_f0,\n",
    "    hz_to_cents,\n",
    "    circular_cents_distance,\n",
    "    rms_cents_error,\n",
    "    estimate_tracking_delay_ms,\n",
    "    compute_overshoot_cents,\n",
    ")\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599372",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"vibrato\": {\n",
    "        \"base_frequency_hz\": 220.0,\n",
    "        \"duration_seconds\": 3.0,\n",
    "        \"rates_hz\": [4.0, 6.0, 8.0],\n",
    "        \"depth_cents\": [20, 60, 120, 200],\n",
    "    },\n",
    "    \"glide\": {\n",
    "        \"start_hz\": 60.0,\n",
    "        \"end_hz\": 500.0,\n",
    "        \"durations_seconds\": [0.4, 0.8, 1.6, 3.2],\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int]:\n",
    "    audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    audio = _ensure_mono(audio)\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    state = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model_state = state.get(\"model\", {})\n",
    "    classifier_weight = model_state.get(\"classifier.weight\") if isinstance(model_state, dict) else None\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(state.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes)\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vibrato_cfg = CONFIG[\"vibrato\"]\n",
    "base_freq = float(vibrato_cfg[\"base_frequency_hz\"])\n",
    "duration = float(vibrato_cfg[\"duration_seconds\"])\n",
    "rates = [float(r) for r in vibrato_cfg[\"rates_hz\"]]\n",
    "depths = [float(d) for d in vibrato_cfg[\"depth_cents\"]]\n",
    "\n",
    "VIBRATO_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for rate in rates:\n",
    "    for depth in depths:\n",
    "        audio, t, f0_curve = generate_vibrato_waveform(rate, depth, base_freq, duration, TARGET_SAMPLE_RATE)\n",
    "        prediction = predict_f0(model, audio)\n",
    "        reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "        metrics = compute_metrics(reference, prediction)\n",
    "        rmse = rms_cents_error(reference, prediction)\n",
    "        VIBRATO_RESULTS.append({\n",
    "            \"rate_hz\": rate,\n",
    "            \"depth_cents\": depth,\n",
    "            \"RPA\": metrics[\"RPA\"],\n",
    "            \"RCA\": metrics[\"RCA\"],\n",
    "            \"VUV\": metrics[\"VUV\"],\n",
    "            \"OctaveError\": metrics[\"OctaveError\"],\n",
    "            \"RMSE_cents\": rmse,\n",
    "        })\n",
    "\n",
    "vibrato_df = pd.DataFrame(VIBRATO_RESULTS)\n",
    "vibrato_df.sort_values([\"rate_hz\", \"depth_cents\"], inplace=True)\n",
    "vibrato_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "\n",
    "for rate in rates:\n",
    "    subset = vibrato_df[vibrato_df[\"rate_hz\"] == rate]\n",
    "    axes[0].plot(subset[\"depth_cents\"], subset[\"RPA\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "    axes[1].plot(subset[\"depth_cents\"], subset[\"RMSE_cents\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "\n",
    "axes[0].set_title(\"RPA vs. Vibrato Depth\")\n",
    "axes[0].set_ylabel(\"RPA\")\n",
    "axes[1].set_title(\"RMSE (cents) vs. Vibrato Depth\")\n",
    "axes[1].set_ylabel(\"RMSE (cents)\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Vibrato depth (cents)\")\n",
    "    ax.set_xticks(depths)\n",
    "    ax.grid(True)\n",
    "axes[-1].legend(title=\"Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glide_cfg = CONFIG[\"glide\"]\n",
    "start_hz = float(glide_cfg[\"start_hz\"])\n",
    "end_hz = float(glide_cfg[\"end_hz\"])\n",
    "durations = [float(d) for d in glide_cfg[\"durations_seconds\"]]\n",
    "\n",
    "GLIDE_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for duration in durations:\n",
    "    audio, t, f0_curve = generate_glide_waveform(duration, start_hz, end_hz, TARGET_SAMPLE_RATE)\n",
    "    prediction = predict_f0(model, audio)\n",
    "    reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "    metrics = compute_metrics(reference, prediction)\n",
    "    rmse = rms_cents_error(reference, prediction)\n",
    "    lag_ms = estimate_tracking_delay_ms(reference, prediction, FRAME_PERIOD_MS)\n",
    "    overshoot = compute_overshoot_cents(reference, prediction)\n",
    "    final_error = float(1200.0 * np.log2(max(prediction[-1], 1e-5) / max(reference[-1], 1e-5))) if prediction.size and reference[-1] > 0 else float(\"nan\")\n",
    "    GLIDE_RESULTS.append({\n",
    "        \"duration_s\": duration,\n",
    "        \"RPA\": metrics[\"RPA\"],\n",
    "        \"RCA\": metrics[\"RCA\"],\n",
    "        \"VUV\": metrics[\"VUV\"],\n",
    "        \"OctaveError\": metrics[\"OctaveError\"],\n",
    "        \"RMSE_cents\": rmse,\n",
    "        \"Lag_ms\": lag_ms,\n",
    "        \"Overshoot_cents\": overshoot,\n",
    "        \"Final_error_cents\": final_error,\n",
    "    })\n",
    "\n",
    "glide_df = pd.DataFrame(GLIDE_RESULTS)\n",
    "glide_df.sort_values(\"duration_s\", inplace=True)\n",
    "glide_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(glide_df[\"duration_s\"], glide_df[\"Lag_ms\"], marker=\"o\")\n",
    "axes[0].set_title(\"Tracking Delay vs. Glide Duration\")\n",
    "axes[0].set_xlabel(\"Duration (s)\")\n",
    "axes[0].set_ylabel(\"Lag (ms)\")\n",
    "\n",
    "axes[1].plot(glide_df[\"duration_s\"], glide_df[\"Overshoot_cents\"], marker=\"o\")\n",
    "axes[1].set_title(\"Overshoot vs. Glide Duration\")\n",
    "axes[1].set_xlabel(\"Duration (s)\")\n",
    "axes[1].set_ylabel(\"Overshoot (cents)\")\n",
    "\n",
    "axes[2].plot(glide_df[\"duration_s\"], glide_df[\"RMSE_cents\"], marker=\"o\")\n",
    "axes[2].set_title(\"RMSE vs. Glide Duration\")\n",
    "axes[2].set_xlabel(\"Duration (s)\")\n",
    "axes[2].set_ylabel(\"RMSE (cents)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0add441",
   "metadata": {},
   "outputs": [],
   "source": [
    "vibrato_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_vibrato_metrics.csv\"\n",
    "vibrato_df.to_csv(vibrato_path, index=False)\n",
    "print(f\"Saved vibrato metrics to {vibrato_path.resolve()}\")\n",
    "\n",
    "\n",
    "glide_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_glide_metrics.csv\"\n",
    "glide_df.to_csv(glide_path, index=False)\n",
    "print(f\"Saved glide metrics to {glide_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}