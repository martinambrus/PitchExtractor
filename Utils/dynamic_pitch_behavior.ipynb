{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Pitch Behavior Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates how the latest pitch extraction model responds to dynamic pitch ",
    "behaviors such as vibrato and portamento/glide transitions. Synthetic stimuli are generated ",
    "with controlled parameters so that we can quantify accuracy, latency, and overshoot across ",
    "a range of condition levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pyworld as pw\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"vibrato\": {\n",
    "        \"base_frequency_hz\": 220.0,\n",
    "        \"duration_seconds\": 3.0,\n",
    "        \"rates_hz\": [4.0, 6.0, 8.0],\n",
    "        \"depth_cents\": [20, 60, 120, 200],\n",
    "    },\n",
    "    \"glide\": {\n",
    "        \"start_hz\": 60.0,\n",
    "        \"end_hz\": 500.0,\n",
    "        \"durations_seconds\": [0.4, 0.8, 1.6, 3.2],\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int]:\n",
    "    audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    audio = _ensure_mono(audio)\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def hz_to_cents(f0: np.ndarray) -> np.ndarray:\n",
    "    cents = np.zeros_like(f0)\n",
    "    positive = f0 > 0\n",
    "    cents[positive] = 1200.0 * np.log2(f0[positive] / 55.0)\n",
    "    return cents\n",
    "\n",
    "\n",
    "def circular_cents_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    diff = a - b\n",
    "    diff = np.mod(diff + 600.0, 1200.0) - 600.0\n",
    "    return diff\n",
    "\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    state = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model_state = state.get(\"model\", {})\n",
    "    classifier_weight = model_state.get(\"classifier.weight\") if isinstance(model_state, dict) else None\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(state.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes)\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _apply_fade(audio: np.ndarray, sr: int, fade_time: float = 0.02) -> np.ndarray:\n",
    "    fade_samples = int(fade_time * sr)\n",
    "    if fade_samples <= 0:\n",
    "        return audio\n",
    "    window = np.ones_like(audio)\n",
    "    ramp = 0.5 - 0.5 * np.cos(np.linspace(0, np.pi, fade_samples))\n",
    "    window[:fade_samples] = ramp\n",
    "    window[-fade_samples:] = ramp[::-1]\n",
    "    return (audio * window).astype(np.float32)\n",
    "\n",
    "\n",
    "def synthesize_from_f0_curve(f0_curve: np.ndarray, sr: int, amplitude: float = 0.8) -> np.ndarray:\n",
    "    omega = 2.0 * np.pi * f0_curve.astype(np.float64) / float(sr)\n",
    "    phase = np.cumsum(omega)\n",
    "    audio = amplitude * np.sin(phase)\n",
    "    audio = _apply_fade(audio.astype(np.float32), sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0.99:\n",
    "        audio = audio / (max_val + 1e-6)\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "\n",
    "def generate_vibrato_waveform(rate_hz: float, depth_cents: float, base_freq: float, duration: float, sr: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    t = np.linspace(0.0, duration, int(duration * sr), endpoint=False)\n",
    "    modulation = np.sin(2.0 * np.pi * rate_hz * t)\n",
    "    f0_curve = base_freq * (2.0 ** ((depth_cents / 1200.0) * modulation))\n",
    "    audio = synthesize_from_f0_curve(f0_curve, sr)\n",
    "    return audio, t, f0_curve\n",
    "\n",
    "\n",
    "def generate_glide_waveform(duration: float, start_hz: float, end_hz: float, sr: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    t = np.linspace(0.0, duration, int(duration * sr), endpoint=False)\n",
    "    f0_curve = np.linspace(start_hz, end_hz, t.shape[0])\n",
    "    audio = synthesize_from_f0_curve(f0_curve, sr)\n",
    "    return audio, t, f0_curve\n",
    "\n",
    "\n",
    "def sample_reference_f0(time_axis: np.ndarray, f0_curve: np.ndarray, num_frames: int) -> np.ndarray:\n",
    "    if num_frames == 0:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    duration = time_axis[-1] + (time_axis[1] - time_axis[0] if time_axis.shape[0] > 1 else 0.0)\n",
    "    frame_times = np.linspace(0.0, duration, num=num_frames, endpoint=False)\n",
    "    reference = np.interp(frame_times, time_axis, f0_curve)\n",
    "    return reference.astype(np.float32)\n",
    "\n",
    "\n",
    "def rms_cents_error(reference: np.ndarray, prediction: np.ndarray) -> float:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    if length == 0:\n",
    "        return float(\"nan\")\n",
    "    ref = reference[:length]\n",
    "    pred = prediction[:length]\n",
    "    mask = ref > 0\n",
    "    if not np.any(mask):\n",
    "        return float(\"nan\")\n",
    "    ref_cents = hz_to_cents(ref[mask])\n",
    "    pred_cents = hz_to_cents(np.clip(pred[mask], a_min=1e-5, a_max=None))\n",
    "    diff = pred_cents - ref_cents\n",
    "    return float(np.sqrt(np.mean(diff ** 2)))\n",
    "\n",
    "\n",
    "def estimate_tracking_delay_ms(reference: np.ndarray, prediction: np.ndarray) -> float:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    if length == 0:\n",
    "        return float(\"nan\")\n",
    "    ref = reference[:length]\n",
    "    pred = prediction[:length]\n",
    "    ref_centered = ref - np.mean(ref)\n",
    "    pred_centered = pred - np.mean(pred)\n",
    "    if np.allclose(ref_centered, 0) or np.allclose(pred_centered, 0):\n",
    "        return float(\"nan\")\n",
    "    corr = np.correlate(pred_centered, ref_centered, mode=\"full\")\n",
    "    lag = np.argmax(corr) - (length - 1)\n",
    "    return float(lag * FRAME_PERIOD_MS)\n",
    "\n",
    "\n",
    "def compute_overshoot_cents(reference: np.ndarray, prediction: np.ndarray) -> float:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    if length == 0:\n",
    "        return float(\"nan\")\n",
    "    ref = reference[:length]\n",
    "    pred = prediction[:length]\n",
    "    target = ref[-1]\n",
    "    peak = np.max(pred) if pred.size else 0.0\n",
    "    if target <= 0 or peak <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float(1200.0 * np.log2(peak / target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "vibrato_cfg = CONFIG[\"vibrato\"]\n",
    "base_freq = float(vibrato_cfg[\"base_frequency_hz\"])\n",
    "duration = float(vibrato_cfg[\"duration_seconds\"])\n",
    "rates = [float(r) for r in vibrato_cfg[\"rates_hz\"]]\n",
    "depths = [float(d) for d in vibrato_cfg[\"depth_cents\"]]\n",
    "\n",
    "VIBRATO_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for rate in rates:\n",
    "    for depth in depths:\n",
    "        audio, t, f0_curve = generate_vibrato_waveform(rate, depth, base_freq, duration, TARGET_SAMPLE_RATE)\n",
    "        prediction = predict_f0(model, audio)\n",
    "        reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "        metrics = compute_metrics(reference, prediction)\n",
    "        rmse = rms_cents_error(reference, prediction)\n",
    "        VIBRATO_RESULTS.append({\n",
    "            \"rate_hz\": rate,\n",
    "            \"depth_cents\": depth,\n",
    "            \"RPA\": metrics[\"RPA\"],\n",
    "            \"RCA\": metrics[\"RCA\"],\n",
    "            \"VUV\": metrics[\"VUV\"],\n",
    "            \"OctaveError\": metrics[\"OctaveError\"],\n",
    "            \"RMSE_cents\": rmse,\n",
    "        })\n",
    "\n",
    "vibrato_df = pd.DataFrame(VIBRATO_RESULTS)\n",
    "vibrato_df.sort_values([\"rate_hz\", \"depth_cents\"], inplace=True)\n",
    "vibrato_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "\n",
    "for rate in rates:\n",
    "    subset = vibrato_df[vibrato_df[\"rate_hz\"] == rate]\n",
    "    axes[0].plot(subset[\"depth_cents\"], subset[\"RPA\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "    axes[1].plot(subset[\"depth_cents\"], subset[\"RMSE_cents\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "\n",
    "axes[0].set_title(\"RPA vs. Vibrato Depth\")\n",
    "axes[0].set_ylabel(\"RPA\")\n",
    "axes[1].set_title(\"RMSE (cents) vs. Vibrato Depth\")\n",
    "axes[1].set_ylabel(\"RMSE (cents)\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Vibrato depth (cents)\")\n",
    "    ax.set_xticks(depths)\n",
    "    ax.grid(True)\n",
    "axes[-1].legend(title=\"Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "glide_cfg = CONFIG[\"glide\"]\n",
    "start_hz = float(glide_cfg[\"start_hz\"])\n",
    "end_hz = float(glide_cfg[\"end_hz\"])\n",
    "durations = [float(d) for d in glide_cfg[\"durations_seconds\"]]\n",
    "\n",
    "GLIDE_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for duration in durations:\n",
    "    audio, t, f0_curve = generate_glide_waveform(duration, start_hz, end_hz, TARGET_SAMPLE_RATE)\n",
    "    prediction = predict_f0(model, audio)\n",
    "    reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "    metrics = compute_metrics(reference, prediction)\n",
    "    rmse = rms_cents_error(reference, prediction)\n",
    "    lag_ms = estimate_tracking_delay_ms(reference, prediction)\n",
    "    overshoot = compute_overshoot_cents(reference, prediction)\n",
    "    final_error = float(1200.0 * np.log2(max(prediction[-1], 1e-5) / max(reference[-1], 1e-5))) if prediction.size and reference[-1] > 0 else float(\"nan\")\n",
    "    GLIDE_RESULTS.append({\n",
    "        \"duration_s\": duration,\n",
    "        \"RPA\": metrics[\"RPA\"],\n",
    "        \"RCA\": metrics[\"RCA\"],\n",
    "        \"VUV\": metrics[\"VUV\"],\n",
    "        \"OctaveError\": metrics[\"OctaveError\"],\n",
    "        \"RMSE_cents\": rmse,\n",
    "        \"Lag_ms\": lag_ms,\n",
    "        \"Overshoot_cents\": overshoot,\n",
    "        \"Final_error_cents\": final_error,\n",
    "    })\n",
    "\n",
    "glide_df = pd.DataFrame(GLIDE_RESULTS)\n",
    "glide_df.sort_values(\"duration_s\", inplace=True)\n",
    "glide_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(glide_df[\"duration_s\"], glide_df[\"Lag_ms\"], marker=\"o\")\n",
    "axes[0].set_title(\"Tracking Delay vs. Glide Duration\")\n",
    "axes[0].set_xlabel(\"Duration (s)\")\n",
    "axes[0].set_ylabel(\"Lag (ms)\")\n",
    "\n",
    "axes[1].plot(glide_df[\"duration_s\"], glide_df[\"Overshoot_cents\"], marker=\"o\")\n",
    "axes[1].set_title(\"Overshoot vs. Glide Duration\")\n",
    "axes[1].set_xlabel(\"Duration (s)\")\n",
    "axes[1].set_ylabel(\"Overshoot (cents)\")\n",
    "\n",
    "axes[2].plot(glide_df[\"duration_s\"], glide_df[\"RMSE_cents\"], marker=\"o\")\n",
    "axes[2].set_title(\"RMSE vs. Glide Duration\")\n",
    "axes[2].set_xlabel(\"Duration (s)\")\n",
    "axes[2].set_ylabel(\"RMSE (cents)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "vibrato_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_vibrato_metrics.csv\"\n",
    "vibrato_df.to_csv(vibrato_path, index=False)\n",
    "print(f\"Saved vibrato metrics to {vibrato_path.resolve()}\")\n",
    "\n",
    "\n",
    "glide_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_glide_metrics.csv\"\n",
    "glide_df.to_csv(glide_path, index=False)\n",
    "print(f\"Saved glide metrics to {glide_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
