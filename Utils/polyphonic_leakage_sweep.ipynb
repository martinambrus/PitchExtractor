{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Polyphonic Leakage Stress Test\n",
        "\n",
        "This notebook evaluates how the trained Joint Detection and Classification (JDC) model handles polyphonic mixtures of vocals and accompaniment. We sweep vocal-to-accompaniment gain ratios and measure vocal activity detection quality together with raw pitch accuracy (RPA) on vocal-active frames. The experiment probes whether the model maintains melody tracking when the accompaniment becomes dominant, revealing failure boundaries for karaoke and singing use cases.\n",
        "\n",
        "**Data expectations**\n",
        "\n",
        "* The evaluation list referenced by `CONFIG['eval_list_path']` should contain one item per line.\n",
        "* Each line must provide at least the vocal stem path. Optionally, supply an accompaniment stem as the second `|`-separated field (e.g., `vocal.wav|accompaniment.wav`).\n",
        "* Paths can be absolute or relative to the evaluation list file.\n",
        "\n",
        "If no accompaniment is provided for an entry, it will be skipped with a warning because the leakage experiment requires both stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import math\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "import pyworld as pw\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from meldataset import DEFAULT_MEL_PARAMS\n",
        "from model import JDCNet\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update the configuration below to point at your training config, checkpoint directory, and evaluation list. The `vocal_to_accompaniment_db` sweep defines the relative gain applied to the vocal stem with respect to the accompaniment (positive values make vocals louder, negative values make accompaniment louder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG: Dict[str, Any] = {\n",
        "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
        "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
        "    \"checkpoint_path\": None,\n",
        "    \"eval_list_path\": None,\n",
        "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\" / \"polyphonic_leakage\",\n",
        "    \"chunk_size\": 192,\n",
        "    \"chunk_overlap\": 48,\n",
        "    \"mel_mean\": -4.0,\n",
        "    \"mel_std\": 4.0,\n",
        "    \"voicing_threshold_hz\": 10.0,\n",
        "    \"vocal_to_accompaniment_db\": list(np.arange(12, -12.1, -3.0)),\n",
        "    \"normalize_mix_peak\": True,\n",
        "}\n",
        "\n",
        "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
        "CONFIG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
        "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
        "\n",
        "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
        "    base_dir = base if base.is_dir() else base.parent\n",
        "    candidate_path = Path(candidate)\n",
        "    if candidate_path.is_absolute():\n",
        "        return candidate_path\n",
        "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
        "    config_candidate = (base_dir / candidate_path).resolve()\n",
        "    if config_candidate.exists():\n",
        "        return config_candidate\n",
        "    return repo_candidate\n",
        "\n",
        "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
        "    if not path.is_dir():\n",
        "        return None\n",
        "    def _sort_key(p: Path) -> tuple[int, float]:\n",
        "        numbers = [int(match) for match in __import__(\"re\").findall(r\"\\d+\", p.stem)]\n",
        "        last = numbers[-1] if numbers else -1\n",
        "        return last, p.stat().st_mtime\n",
        "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
        "    return candidates[-1] if candidates else None\n",
        "\n",
        "def _load_training_config() -> Dict[str, Any]:\n",
        "    config_path = CONFIG.get(\"config_path\")\n",
        "    if not config_path or not Path(config_path).is_file():\n",
        "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
        "        return {}\n",
        "    import yaml\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
        "        data = yaml.safe_load(handle) or {}\n",
        "    dataset_params = data.get(\"dataset_params\", {})\n",
        "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
        "    dataset_sr = dataset_params.get(\"sr\")\n",
        "    if dataset_sr is not None:\n",
        "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
        "    val_list = data.get(\"val_data\")\n",
        "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
        "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
        "        if not CONFIG[\"eval_list_path\"].is_file():\n",
        "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
        "    log_dir = data.get(\"log_dir\")\n",
        "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
        "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
        "    if CONFIG.get(\"checkpoint_path\") is None:\n",
        "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
        "        if latest is not None:\n",
        "            CONFIG[\"checkpoint_path\"] = latest\n",
        "        else:\n",
        "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
        "    return data\n",
        "\n",
        "TRAINING_CONFIG = _load_training_config()\n",
        "TARGET_SAMPLE_RATE = int(MEL_PARAMS[\"sample_rate\"])\n",
        "HOP_LENGTH = int(MEL_PARAMS[\"hop_length\"])\n",
        "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
        "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
        "\n",
        "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
        "    if audio.ndim == 1:\n",
        "        return audio\n",
        "    return audio.mean(axis=1)\n",
        "\n",
        "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int]:\n",
        "    audio, sr = sf.read(str(path), dtype=\"float32\")\n",
        "    audio = _ensure_mono(audio)\n",
        "    if sr != target_sr:\n",
        "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
        "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
        "        audio = resampled.squeeze(0).cpu().numpy()\n",
        "        sr = target_sr\n",
        "    return audio.astype(np.float32), sr\n",
        "\n",
        "def compute_reference_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
        "    if audio.size == 0:\n",
        "        return np.zeros((0,), dtype=np.float32)\n",
        "    audio64 = audio.astype(\"double\")\n",
        "    f0, t = pw.harvest(audio64, sr, frame_period=FRAME_PERIOD_MS)\n",
        "    if np.count_nonzero(f0) < 5:\n",
        "        f0, t = pw.dio(audio64, sr, frame_period=FRAME_PERIOD_MS)\n",
        "    refined = pw.stonemask(audio64, f0, t, sr)\n",
        "    return refined.astype(np.float32)\n",
        "\n",
        "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
        "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        mel = mel_transform(tensor)\n",
        "    mel = torch.log(mel + 1e-5)\n",
        "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
        "    return mel.squeeze(0)\n",
        "\n",
        "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
        "    mel = waveform_to_mel(audio)\n",
        "    total_frames = mel.shape[-1]\n",
        "    chunk_size = int(CONFIG[\"chunk_size\"])\n",
        "    overlap = int(CONFIG[\"chunk_overlap\"])\n",
        "    step = max(chunk_size - overlap, 1)\n",
        "    preds: List[np.ndarray] = []\n",
        "    for start in range(0, total_frames, step):\n",
        "        end = min(start + chunk_size, total_frames)\n",
        "        mel_chunk = mel[:, start:end]\n",
        "        pad = chunk_size - mel_chunk.shape[-1]\n",
        "        if pad > 0:\n",
        "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
        "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
        "        with torch.no_grad():\n",
        "            f0_chunk, _ = model(mel_chunk)\n",
        "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
        "        preds.append(f0_chunk[: end - start])\n",
        "    if preds:\n",
        "        return np.concatenate(preds)\n",
        "    return np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "def hz_to_cents(f0: np.ndarray) -> np.ndarray:\n",
        "    cents = np.zeros_like(f0)\n",
        "    positive = f0 > 0\n",
        "    cents[positive] = 1200.0 * np.log2(f0[positive] / 55.0)\n",
        "    return cents.astype(np.float32)\n",
        "\n",
        "def circular_cents_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    diff = a - b\n",
        "    diff = np.mod(diff + 600.0, 1200.0) - 600.0\n",
        "    return diff\n",
        "\n",
        "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
        "    length = min(reference.shape[0], prediction.shape[0])\n",
        "    reference = reference[:length]\n",
        "    prediction = prediction[:length]\n",
        "    ref_voiced = reference > 0\n",
        "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
        "    total_frames = max(length, 1)\n",
        "    voiced_frames = max(int(np.count_nonzero(ref_voiced)), 1)\n",
        "    tp = np.count_nonzero(ref_voiced & pred_voiced)\n",
        "    fp = np.count_nonzero(~ref_voiced & pred_voiced)\n",
        "    fn = np.count_nonzero(ref_voiced & ~pred_voiced)\n",
        "    precision = float(tp / (tp + fp)) if (tp + fp) > 0 else float(\"nan\")\n",
        "    recall = float(tp / (tp + fn)) if (tp + fn) > 0 else float(\"nan\")\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2.0 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = float(\"nan\")\n",
        "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / total_frames)\n",
        "    if np.count_nonzero(ref_voiced) == 0:\n",
        "        return {\n",
        "            \"RPA\": float(\"nan\"),\n",
        "            \"RCA\": float(\"nan\"),\n",
        "            \"VUV\": vuv_accuracy,\n",
        "            \"VAD_Precision\": precision,\n",
        "            \"VAD_Recall\": recall,\n",
        "            \"VAD_F1\": f1,\n",
        "            \"OctaveError\": float(\"nan\"),\n",
        "            \"VoicedFrames\": 0.0,\n",
        "        }\n",
        "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
        "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
        "    cents_diff = pred_cents - ref_cents\n",
        "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
        "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
        "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
        "    octave_candidates = np.abs(cents_diff) > 50.0\n",
        "    octave_numbers = np.round(cents_diff / 1200.0)\n",
        "    octave_errors = octave_candidates & (octave_numbers != 0) & (\n",
        "        np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0\n",
        "    )\n",
        "    return {\n",
        "        \"RPA\": float(np.count_nonzero(rpa_hits) / np.count_nonzero(ref_voiced)),\n",
        "        \"RCA\": float(np.count_nonzero(rca_hits) / np.count_nonzero(ref_voiced)),\n",
        "        \"VUV\": vuv_accuracy,\n",
        "        \"VAD_Precision\": precision,\n",
        "        \"VAD_Recall\": recall,\n",
        "        \"VAD_F1\": f1,\n",
        "        \"OctaveError\": float(np.count_nonzero(octave_errors) / np.count_nonzero(ref_voiced)),\n",
        "        \"VoicedFrames\": float(np.count_nonzero(ref_voiced)),\n",
        "    }\n",
        "\n",
        "def _match_length(audio: np.ndarray, target_length: int) -> np.ndarray:\n",
        "    if audio.shape[0] == target_length:\n",
        "        return audio\n",
        "    if audio.shape[0] > target_length:\n",
        "        return audio[:target_length]\n",
        "    return np.pad(audio, (0, target_length - audio.shape[0]))\n",
        "\n",
        "def mix_vocals_with_accompaniment(\n",
        "    vocal: np.ndarray,\n",
        "    accompaniment: Optional[np.ndarray],\n",
        "    ratio_db: float,\n",
        "    normalize_peak: bool = True,\n",
        ") -> np.ndarray:\n",
        "    if accompaniment is None or accompaniment.size == 0:\n",
        "        warnings.warn(\"Missing accompaniment; returning scaled vocals only.\")\n",
        "        accompaniment = np.zeros_like(vocal)\n",
        "    target_length = max(vocal.shape[0], accompaniment.shape[0])\n",
        "    vocal_aligned = _match_length(vocal, target_length)\n",
        "    accomp_aligned = _match_length(accompaniment, target_length)\n",
        "    vocal_gain = 10.0 ** (ratio_db / 20.0)\n",
        "    mix = vocal_gain * vocal_aligned + accomp_aligned\n",
        "    if normalize_peak and mix.size > 0:\n",
        "        peak = np.max(np.abs(mix))\n",
        "        if peak > 0.99:\n",
        "            mix = mix / (peak + 1e-6)\n",
        "    return mix.astype(np.float32)\n",
        "\n",
        "DATASET_CACHE: Optional[List[Dict[str, Any]]] = None\n",
        "\n",
        "def prepare_dataset_cache(force: bool = False) -> List[Dict[str, Any]]:\n",
        "    global DATASET_CACHE\n",
        "    if DATASET_CACHE is not None and not force:\n",
        "        return DATASET_CACHE\n",
        "    eval_list = CONFIG.get(\"eval_list_path\")\n",
        "    if eval_list is None or not Path(eval_list).is_file():\n",
        "        raise FileNotFoundError(f\"Evaluation list not found: {eval_list}\")\n",
        "    entries: List[Dict[str, Optional[Path]]] = []\n",
        "    with open(eval_list, \"r\", encoding=\"utf-8\") as handle:\n",
        "        for raw_line in handle:\n",
        "            line = raw_line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = [part.strip() for part in line.split(\"|\")]\n",
        "            vocal_candidate = Path(parts[0])\n",
        "            if not vocal_candidate.is_absolute():\n",
        "                vocal_candidate = (Path(eval_list).parent / vocal_candidate).resolve()\n",
        "            accompaniment_candidate = None\n",
        "            if len(parts) > 1 and parts[1]:\n",
        "                accompaniment_candidate = Path(parts[1])\n",
        "                if not accompaniment_candidate.is_absolute():\n",
        "                    accompaniment_candidate = (Path(eval_list).parent / accompaniment_candidate).resolve()\n",
        "            entries.append({\n",
        "                \"vocal\": vocal_candidate,\n",
        "                \"accompaniment\": accompaniment_candidate,\n",
        "            })\n",
        "    if not entries:\n",
        "        raise RuntimeError(f\"No evaluation files located in {eval_list}\")\n",
        "    cache: List[Dict[str, Any]] = []\n",
        "    skipped = 0\n",
        "    for entry in tqdm(entries, desc=\"Preparing evaluation cache\"):\n",
        "        vocal_path = entry[\"vocal\"]\n",
        "        accompaniment_path = entry.get(\"accompaniment\")\n",
        "        if accompaniment_path is None or not accompaniment_path.is_file():\n",
        "            warnings.warn(f\"Skipping {vocal_path} because accompaniment is missing.\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "        vocal_audio, sr = load_waveform(vocal_path)\n",
        "        accompaniment_audio, accomp_sr = load_waveform(accompaniment_path)\n",
        "        if accomp_sr != sr:\n",
        "            accompaniment_audio, _ = load_waveform(accompaniment_path, sr)\n",
        "        reference_f0 = compute_reference_f0(vocal_audio, sr)\n",
        "        cache.append({\n",
        "            \"path\": vocal_path,\n",
        "            \"vocal_audio\": vocal_audio,\n",
        "            \"accompaniment_audio\": accompaniment_audio,\n",
        "            \"sample_rate\": sr,\n",
        "            \"reference_f0\": reference_f0,\n",
        "        })\n",
        "    if not cache:\n",
        "        raise RuntimeError(\"No evaluation items with both vocal and accompaniment were found.\")\n",
        "    if skipped:\n",
        "        print(f\"Warning: skipped {skipped} entries without accompaniment stems.\")\n",
        "    DATASET_CACHE = cache\n",
        "    print(f\"Cached {len(DATASET_CACHE)} evaluation utterances.\")\n",
        "    return DATASET_CACHE\n",
        "\n",
        "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
        "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
        "    if not checkpoint_path.is_file():\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "    state = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    model_state = state.get(\"model\", {})\n",
        "    classifier_weight = model_state.get(\"classifier.weight\") if isinstance(model_state, dict) else None\n",
        "    if classifier_weight is not None:\n",
        "        inferred_classes = int(classifier_weight.shape[0])\n",
        "    else:\n",
        "        inferred_classes = int(state.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
        "    if inferred_classes <= 0:\n",
        "        inferred_classes = 722\n",
        "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
        "    model = JDCNet(num_class=inferred_classes)\n",
        "    model.load_state_dict(model_state)\n",
        "    model.to(DEVICE).eval()\n",
        "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model and evaluation cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model()\n",
        "DATASET_CACHE = prepare_dataset_cache(force=True)\n",
        "len(DATASET_CACHE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: Vocal-only performance\n",
        "\n",
        "We first evaluate the model on clean vocal stems to establish a reference for RPA and vocal activity detection quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_records: List[Dict[str, Any]] = []\n",
        "for entry in tqdm(DATASET_CACHE, desc=\"Evaluating vocal-only baseline\"):\n",
        "    prediction = predict_f0(model, entry[\"vocal_audio\"])\n",
        "    metrics = compute_metrics(entry[\"reference_f0\"], prediction)\n",
        "    baseline_records.append({\"path\": str(entry[\"path\"]), **metrics})\n",
        "baseline_df = pd.DataFrame(baseline_records)\n",
        "baseline_summary = baseline_df[[\"RPA\", \"VAD_F1\", \"VAD_Recall\", \"VAD_Precision\", \"VUV\"]].mean()\n",
        "display(baseline_summary.to_frame(name=\"Baseline\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polyphonic leakage sweep\n",
        "\n",
        "Each condition mixes the vocal and accompaniment stems according to the specified gain ratio. We then evaluate RPA and vocal activity detection metrics on the resulting mixtures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_results: List[Dict[str, Any]] = []\n",
        "for ratio_db in CONFIG[\"vocal_to_accompaniment_db\"]:\n",
        "    for entry in tqdm(DATASET_CACHE, desc=f\"Mixing {ratio_db:+.1f} dB\", leave=False):\n",
        "        mixture = mix_vocals_with_accompaniment(\n",
        "            entry[\"vocal_audio\"],\n",
        "            entry[\"accompaniment_audio\"],\n",
        "            ratio_db,\n",
        "            normalize_peak=CONFIG.get(\"normalize_mix_peak\", True),\n",
        "        )\n",
        "        prediction = predict_f0(model, mixture)\n",
        "        metrics = compute_metrics(entry[\"reference_f0\"], prediction)\n",
        "        sweep_results.append({\n",
        "            \"ratio_db\": float(ratio_db),\n",
        "            \"path\": str(entry[\"path\"]),\n",
        "            **metrics,\n",
        "        })\n",
        "polyphonic_df = pd.DataFrame(sweep_results)\n",
        "polyphonic_summary = (\n",
        "    polyphonic_df\n",
        "    .groupby(\"ratio_db\")[[\"RPA\", \"VAD_F1\", \"VAD_Recall\", \"VAD_Precision\", \"VUV\"]]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .sort_values(\"ratio_db\", ascending=False)\n",
        ")\n",
        "display(polyphonic_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise failure boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not polyphonic_summary.empty:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True)\n",
        "    axes[0].plot(polyphonic_summary[\"ratio_db\"], polyphonic_summary[\"RPA\"], marker=\"o\")\n",
        "    axes[0].axhline(baseline_summary[\"RPA\"], color=\"gray\", linestyle=\"--\", label=\"Baseline\")\n",
        "    axes[0].set_xlabel(\"Vocal : Accompaniment (dB)\")\n",
        "    axes[0].set_ylabel(\"RPA\")\n",
        "    axes[0].set_title(\"Raw Pitch Accuracy vs Mix Ratio\")\n",
        "    axes[0].invert_xaxis()\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(polyphonic_summary[\"ratio_db\"], polyphonic_summary[\"VAD_F1\"], marker=\"o\", label=\"F1\")\n",
        "    axes[1].plot(polyphonic_summary[\"ratio_db\"], polyphonic_summary[\"VAD_Recall\"], marker=\"s\", label=\"Recall\")\n",
        "    axes[1].plot(polyphonic_summary[\"ratio_db\"], polyphonic_summary[\"VAD_Precision\"], marker=\"^\", label=\"Precision\")\n",
        "    axes[1].axhline(baseline_summary[\"VAD_F1\"], color=\"gray\", linestyle=\"--\", label=\"Baseline F1\")\n",
        "    axes[1].set_xlabel(\"Vocal : Accompaniment (dB)\")\n",
        "    axes[1].set_ylabel(\"Score\")\n",
        "    axes[1].set_title(\"Vocal Activity Detection Metrics\")\n",
        "    axes[1].invert_xaxis()\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No sweep results available for plotting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export artefacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "baseline_df.to_csv(OUTPUT_DIR / \"baseline_vocal_metrics.csv\", index=False)\n",
        "polyphonic_df.to_csv(OUTPUT_DIR / \"polyphonic_leakage_metrics.csv\", index=False)\n",
        "polyphonic_summary.to_csv(OUTPUT_DIR / \"polyphonic_leakage_summary.csv\", index=False)\n",
        "print(f\"Artifacts written to {OUTPUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}