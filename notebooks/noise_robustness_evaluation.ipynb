{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9194d68d",
   "metadata": {},
   "source": [
    "\n",
    "# Noise Robustness Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the noise robustness of a pitch extraction model trained with the JDC-PitchExtractor repository. It runs additive noise sweeps for several real-world noise types and reports standard melody extraction metrics (RPA, RCA, V/UV accuracy, and octave error rate).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1fe46",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "Uncomment and run the cell below if the required dependencies are not yet installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326dfc7",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and Global Configuration\n",
    "\n",
    "The repository root is added to `sys.path` so the notebook can reuse the dataset and model utilities that ship with the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pyworld as pw\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac6af6",
   "metadata": {},
   "source": [
    "\n",
    "## User Configuration\n",
    "\n",
    "Update the configuration dictionary in the next cell with paths specific to your environment:\n",
    "\n",
    "* `config_path`: Training configuration used for the model (for mel parameters and hop length).\n",
    "* `checkpoint_path`: Trained model checkpoint to evaluate.\n",
    "* `eval_list_path`: Text file listing evaluation audio files (one `path|metadata` per line, as used during training).\n",
    "* `noise_library`: Mapping of noise type to a list of background-noise WAV files. White and pink noise are generated procedurally; for babble, cafe, and HVAC provide representative recordings.\n",
    "* `output_dir`: Folder used to cache intermediate artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"eval_list_path\": None,\n",
    "    \"noise_library\": {\n",
    "        \"white\": [],  # generated procedurally\n",
    "        \"pink\": [],   # generated procedurally\n",
    "        \"babble\": [REPO_ROOT / \"NoiseLibrary\" / \"babble.wav\"],\n",
    "        \"cafe\": [REPO_ROOT / \"NoiseLibrary\" / \"cafe.wav\"],\n",
    "        \"HVAC\": [REPO_ROOT / \"NoiseLibrary\" / \"hvac.wav\"],\n",
    "    },\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"snr_levels\": list(range(30, -5, -5)),\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bace256",
   "metadata": {},
   "source": [
    "\n",
    "## Helper Utilities\n",
    "\n",
    "The cell below defines audio utilities, inference helpers, and metric computations used in the evaluation loop. They closely follow the preprocessing steps used during training (`meldataset.MelDataset`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Path | None:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _checkpoint_sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last_number = numbers[-1] if numbers else -1\n",
    "        return last_number, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_checkpoint_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "if CONFIG[\"config_path\"].is_file():\n",
    "    import yaml\n",
    "\n",
    "    with open(CONFIG[\"config_path\"], \"r\", encoding=\"utf-8\") as config_file:\n",
    "        training_config = yaml.safe_load(config_file) or {}\n",
    "\n",
    "    MEL_PARAMS.update(training_config.get(\"dataset_params\", {}).get(\"mel_params\", {}))\n",
    "    dataset_sr = training_config.get(\"dataset_params\", {}).get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_data = training_config.get(\"val_data\")\n",
    "    if val_data:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(CONFIG[\"config_path\"], val_data)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "    else:\n",
    "        print(\"Warning: validation list not defined in config; set CONFIG['eval_list_path'] manually.\")\n",
    "\n",
    "    log_dir = training_config.get(\"log_dir\")\n",
    "    if log_dir:\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(CONFIG[\"config_path\"], log_dir)\n",
    "    elif not CONFIG[\"checkpoint_dir\"].is_dir():\n",
    "        print(\"Warning: log_dir missing in config and default checkpoint directory does not exist.\")\n",
    "\n",
    "    latest_checkpoint = _latest_checkpoint(CONFIG[\"checkpoint_dir\"])\n",
    "    if latest_checkpoint is not None:\n",
    "        CONFIG[\"checkpoint_path\"] = latest_checkpoint\n",
    "    else:\n",
    "        print(f\"Warning: no checkpoint files found in {CONFIG['checkpoint_dir']}. Set CONFIG['checkpoint_path'] manually.\")\n",
    "else:\n",
    "    print(\"Warning: config file not found, falling back to DEFAULT_MEL_PARAMS\")\n",
    "\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> Tuple[np.ndarray, int]:\n",
    "    audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    audio = _ensure_mono(audio)\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def compute_reference_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    if audio.size == 0:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    audio64 = audio.astype(\"double\")\n",
    "    f0, t = pw.harvest(audio64, sr, frame_period=FRAME_PERIOD_MS)\n",
    "    if np.count_nonzero(f0) < 5:\n",
    "        f0, t = pw.dio(audio64, sr, frame_period=FRAME_PERIOD_MS)\n",
    "    refined = pw.stonemask(audio64, f0, t, sr)\n",
    "    return refined.astype(np.float32)\n",
    "\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "\n",
    "    preds = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0)\n",
    "        mel_chunk = mel_chunk.transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def voss_mccartney_pink(length: int) -> np.ndarray:\n",
    "    num_rows = 16\n",
    "    random_state = np.random.randn(num_rows, length).astype(np.float32)\n",
    "    cumulative = np.cumsum(random_state, axis=1)\n",
    "    weighting = 2.0 ** np.arange(num_rows, dtype=np.float32)[:, None]\n",
    "    pink = (cumulative / weighting).sum(axis=0)\n",
    "    pink /= np.max(np.abs(pink) + 1e-12)\n",
    "    return pink.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_noise_sample(noise_type: str, length: int) -> np.ndarray:\n",
    "    library = CONFIG[\"noise_library\"].get(noise_type, [])\n",
    "    if noise_type in {\"white\", \"pink\"} or library:\n",
    "        if noise_type == \"white\":\n",
    "            sample = np.random.randn(length).astype(np.float32)\n",
    "        elif noise_type == \"pink\":\n",
    "            sample = voss_mccartney_pink(length)\n",
    "        else:\n",
    "            choices = [Path(p) for p in library if Path(p).is_file()]\n",
    "            if not choices:\n",
    "                raise FileNotFoundError(f\"No audio files found for noise type '{noise_type}'.\")\n",
    "            path = np.random.choice(choices)\n",
    "            audio, _ = load_waveform(path, TARGET_SAMPLE_RATE)\n",
    "            if audio.shape[0] < length:\n",
    "                reps = math.ceil(length / audio.shape[0])\n",
    "                audio = np.tile(audio, reps)\n",
    "            start = np.random.randint(0, max(1, audio.shape[0] - length + 1))\n",
    "            sample = audio[start:start + length]\n",
    "        return sample.astype(np.float32)\n",
    "    raise ValueError(f\"Noise type '{noise_type}' has no associated samples.\")\n",
    "\n",
    "\n",
    "def mix_snr(clean: np.ndarray, noise: np.ndarray, snr_db: float) -> np.ndarray:\n",
    "    if noise.shape[0] < clean.shape[0]:\n",
    "        reps = math.ceil(clean.shape[0] / noise.shape[0])\n",
    "        noise = np.tile(noise, reps)\n",
    "    noise = noise[: clean.shape[0]]\n",
    "    clean_rms = np.sqrt(np.mean(clean ** 2) + 1e-12)\n",
    "    noise_rms = np.sqrt(np.mean(noise ** 2) + 1e-12)\n",
    "    desired_noise_rms = clean_rms / (10 ** (snr_db / 20))\n",
    "    scaled_noise = noise * (desired_noise_rms / noise_rms)\n",
    "    return clean + scaled_noise\n",
    "\n",
    "\n",
    "def hz_to_cents(f0: np.ndarray) -> np.ndarray:\n",
    "    cents = np.zeros_like(f0)\n",
    "    positive = f0 > 0\n",
    "    cents[positive] = 1200.0 * np.log2(f0[positive] / 55.0)\n",
    "    return cents\n",
    "\n",
    "\n",
    "def circular_cents_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    diff = a - b\n",
    "    diff = np.mod(diff + 600.0, 1200.0) - 600.0\n",
    "    return diff\n",
    "\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "\n",
    "    total_frames = length\n",
    "    voiced_frames = np.count_nonzero(ref_voiced)\n",
    "\n",
    "    vuv_accuracy = np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1)\n",
    "\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "\n",
    "    return {\n",
    "        \"RPA\": np.count_nonzero(rpa_hits) / voiced_frames,\n",
    "        \"RCA\": np.count_nonzero(rca_hits) / voiced_frames,\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": np.count_nonzero(octave_errors) / voiced_frames,\n",
    "    }\n",
    "\n",
    "DATASET_CACHE: list[dict[str, Any]] | None = None\n",
    "\n",
    "\n",
    "def prepare_dataset_cache(force: bool = False) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load evaluation waveforms and ground-truth f0s, caching results.\"\"\"\n",
    "    global DATASET_CACHE\n",
    "    if DATASET_CACHE is not None and not force:\n",
    "        return DATASET_CACHE\n",
    "\n",
    "    if CONFIG[\"eval_list_path\"] is None or not Path(CONFIG[\"eval_list_path\"]).is_file():\n",
    "        raise FileNotFoundError(f\"Evaluation list not found: {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    eval_entries: List[Path] = []\n",
    "    with open(CONFIG[\"eval_list_path\"], \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            relative_path = line.split(\"|\")[0]\n",
    "            path = Path(relative_path)\n",
    "            if not path.is_absolute():\n",
    "                path = (CONFIG[\"eval_list_path\"].parent / path).resolve()\n",
    "            eval_entries.append(path)\n",
    "\n",
    "    if not eval_entries:\n",
    "        raise RuntimeError(f\"No evaluation files found in {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    cache: list[dict[str, Any]] = []\n",
    "    for path in tqdm(eval_entries, desc=\"Preparing evaluation cache\"):\n",
    "        audio, sr = load_waveform(path)\n",
    "        ref_f0 = compute_reference_f0(audio, sr)\n",
    "        cache.append(\n",
    "            {\n",
    "                \"path\": path,\n",
    "                \"audio\": audio,\n",
    "                \"sample_rate\": sr,\n",
    "                \"reference_f0\": ref_f0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    DATASET_CACHE = cache\n",
    "    print(f\"Cached {len(DATASET_CACHE)} evaluation utterances.\")\n",
    "    return DATASET_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba4924",
   "metadata": {},
   "source": [
    "\n",
    "## Load Model and Evaluation Set\n",
    "\n",
    "This cell loads the trained checkpoint, moves the model to the selected device, and prepares the evaluation dataset by caching the clean waveforms and reference F0 tracks generated with WORLD vocoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"checkpoint_path\"] is None or not Path(CONFIG[\"checkpoint_path\"]).is_file():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CONFIG['checkpoint_path']}\")\n",
    "\n",
    "model = JDCNet(num_class=1)\n",
    "state = torch.load(CONFIG[\"checkpoint_path\"], map_location=\"cpu\")\n",
    "model.load_state_dict(state[\"model\"] if \"model\" in state else state)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Loaded checkpoint from {CONFIG['checkpoint_path']}\")\n",
    "\n",
    "prepare_dataset_cache(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee33d9c",
   "metadata": {},
   "source": [
    "\n",
    "## Noise Robustness Sweep\n",
    "\n",
    "For each noise type and signal-to-noise ratio (SNR) level, the notebook adds noise to the cached waveforms, runs the model, and aggregates metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULTS = []\n",
    "noise_types = list(CONFIG[\"noise_library\"].keys())\n",
    "metrics = [\"RPA\", \"RCA\", \"VUV\", \"OctaveError\"]\n",
    "\n",
    "dataset_cache = prepare_dataset_cache()\n",
    "for noise_type in noise_types:\n",
    "    for snr_db in CONFIG[\"snr_levels\"]:\n",
    "        metric_totals = {metric: [] for metric in metrics}\n",
    "        for entry in tqdm(dataset_cache, desc=f\"{noise_type} @ {snr_db} dB\", leave=False):\n",
    "            noisy = mix_snr(entry[\"audio\"], load_noise_sample(noise_type, entry[\"audio\"].shape[0]), snr_db)\n",
    "            prediction = predict_f0(model, noisy)\n",
    "            metric_values = compute_metrics(entry[\"reference_f0\"], prediction)\n",
    "            for metric in metrics:\n",
    "                metric_totals[metric].append(metric_values[metric])\n",
    "        for metric in metrics:\n",
    "            scores = np.array(metric_totals[metric], dtype=np.float32)\n",
    "            RESULTS.append({\n",
    "                \"noise_type\": noise_type,\n",
    "                \"snr_db\": snr_db,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": float(np.nanmean(scores)),\n",
    "                \"std\": float(np.nanstd(scores)),\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(RESULTS)\n",
    "results_df.sort_values([\"metric\", \"noise_type\", \"snr_db\"], inplace=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca002ca5",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization\n",
    "\n",
    "The following plots show each metric as a function of SNR for every noise condition. The shaded band corresponds to one standard deviation across the evaluation utterances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(4 * len(metrics), 4), sharex=True)\n",
    "if len(metrics) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    for noise_type in noise_types:\n",
    "        subset = results_df[(results_df[\"metric\"] == metric) & (results_df[\"noise_type\"] == noise_type)]\n",
    "        subset = subset.sort_values(\"snr_db\")\n",
    "        ax.plot(subset[\"snr_db\"], subset[\"mean\"], marker=\"o\", label=noise_type)\n",
    "        ax.fill_between(subset[\"snr_db\"], subset[\"mean\"] - subset[\"std\"], subset[\"mean\"] + subset[\"std\"], alpha=0.2)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"SNR (dB)\")\n",
    "    ax.set_ylim(0.0, 1.05)\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[-1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410ff8f",
   "metadata": {},
   "source": [
    "\n",
    "## Saving Results\n",
    "\n",
    "Persist the aggregated metrics to disk so they can be consumed by external reporting pipelines or manuscripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_path = CONFIG[\"output_dir\"] / \"noise_robustness_metrics.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Saved results to {results_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
